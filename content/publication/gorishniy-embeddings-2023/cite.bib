@misc{gorishniy_embeddings_2023,
 abstract = {Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial for many backbones, not only for Transformers. Specifically, after proper embeddings, simple MLP-like models can perform on par with the attention-based architectures. Overall, we highlight embeddings for numerical features as an important design aspect with good potential for further improvements in tabular DL.},
 author = {Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
 date = {2023-07-26},
 doi = {10.48550/arXiv.2203.05556},
 eprint = {2203.05556 [cs]},
 eprinttype = {arxiv},
 file = {arXiv Fulltext PDF:D\:\\Zetero\\storage\\RS934UAY\\Gorishniy et al. - 2023 - On Embeddings for Numerical Features in Tabular De.pdf:application/pdf;arXiv.org Snapshot:D\:\\Zetero\\storage\\D9GXHIHQ\\2203.html:text/html},
 keywords = {Computer Science - Machine Learning},
 number = {arXiv:2203.05556},
 publisher = {arXiv},
 title = {On Embeddings for Numerical Features in Tabular Deep Learning},
 url = {http://arxiv.org/abs/2203.05556},
 urldate = {2023-08-01}
}
