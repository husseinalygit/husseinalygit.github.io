@misc{chen_targeted_2017,
 abstract = {Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.},
 author = {Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
 date = {2017-12-14},
 doi = {10.48550/arXiv.1712.05526},
 eprint = {1712.05526 [cs]},
 eprinttype = {arxiv},
 file = {arXiv Fulltext PDF:D\:\\Zetero\\storage\\VAKDKEQZ\\Chen et al. - 2017 - Targeted Backdoor Attacks on Deep Learning Systems.pdf:application/pdf;arXiv.org Snapshot:D\:\\Zetero\\storage\\YZAQ9XH8\\1712.html:text/html},
 keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
 number = {arXiv:1712.05526},
 publisher = {arXiv},
 title = {Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning},
 url = {http://arxiv.org/abs/1712.05526},
 urldate = {2023-08-03}
}
