@inproceedings{carlini_is_2021,
 abstract = {A private machine learning algorithm hides as much as possible about its training data while still preserving accuracy. In this work, we study whether a non-private learning algorithm can be made private by relying on an instance-encoding mechanism that modifies the training inputs before feeding them to a normal learner. We formalize both the notion of instance encoding and its privacy by providing two attack models. We first prove impossibility results for achieving a (stronger) model. Next, we demonstrate practical attacks in the second (weaker) attack model on InstaHide, a recent proposal by Huang, Song, Li and Arora [ICML’20] that aims to use instance encoding for privacy.},
 author = {Carlini, Nicholas and Deng, Samuel and Garg, Sanjam and Jha, Somesh and Mahloujifar, Saeed and Mahmoody, Mohammad and Thakurta, Abhradeep and Tramèr, Florian},
 booktitle = {2021 IEEE Symposium on Security and Privacy (SP)},
 date = {2021-05},
 doi = {10.1109/SP40001.2021.00099},
 eventtitle = {2021 IEEE Symposium on Security and Privacy (SP)},
 file = {Carlini et al_2021_Is Private Learning Possible with Instance Encoding.pdf:D\:\\Zetero\\storage\\4CW3N8RD\\Carlini et al_2021_Is Private Learning Possible with Instance Encoding.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\Zetero\\storage\\ZSDHEB8Q\\9519450.html:text/html},
 keywords = {Training, Computational modeling, Neural networks, Privacy, Training data, Encoding, Machine learning algorithms},
 note = {ISSN: 2375-1207},
 pages = {410--427},
 title = {Is Private Learning Possible with Instance Encoding?}
}
