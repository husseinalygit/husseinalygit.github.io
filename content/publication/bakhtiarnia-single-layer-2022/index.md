---
title: Single-layer vision transformers for more accurate early exits with less overhead
authors:
- Arian Bakhtiarnia
- Qi Zhang
- Alexandros Iosifidis
date: '2022-09-01'
publishDate: '2023-11-24T10:39:01.222731Z'
publication_types:
- article-journal
doi: 10.1016/j.neunet.2022.06.038
abstract: Deploying deep learning models in time-critical applications with limited
  computational resources, for instance in edge computing systems and IoT networks,
  is a challenging task that often relies on dynamic inference methods such as early
  exiting. In this paper, we introduce a novel architecture for early exiting based
  on the vision transformer architecture, as well as a fine-tuning strategy that significantly
  increase the accuracy of early exit branches compared to conventional approaches
  while introducing less overhead. Through extensive experiments on image and audio
  classification as well as audiovisual crowd counting, we show that our method works
  for both classification and regression problems, and in both single- and multi-modal
  settings. Additionally, we introduce a novel method for integrating audio and visual
  modalities within early exits in audiovisual data analysis, that can lead to a more
  fine-grained dynamic inference.
tags:
- Dynamic inference
- Early exiting
- Multi-exit architecture
- Multimodal deep learning
- Vision transformer
links:
- name: URL
  url: https://www.sciencedirect.com/science/article/pii/S0893608022002532
---
