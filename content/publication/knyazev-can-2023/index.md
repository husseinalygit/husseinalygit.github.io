---
title: Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?
authors:
- Boris Knyazev
- Doha Hwang
- Simon Lacoste-Julien
date: '2023-05-31'
publishDate: '2023-11-24T10:39:01.515009Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2303.04143
abstract: Pretraining a neural network on a large dataset is becoming a cornerstone
  in machine learning that is within the reach of only a few communities with large-resources.
  We aim at an ambitious goal of democratizing pretraining. Towards that goal, we
  train and release a single neural network that can predict high quality ImageNet
  parameters of other neural networks. By using predicted parameters for initialization
  we are able to boost training of diverse ImageNet models available in PyTorch. When
  transferred to other datasets, models initialized with predicted parameters also
  converge faster and reach competitive final performance.
tags:
- Computer Science - Machine Learning
- Computer Science - Computer Vision and Pattern Recognition
- Computer Science - Artificial Intelligence
- Statistics - Machine Learning
links:
- name: URL
  url: http://arxiv.org/abs/2303.04143
---
