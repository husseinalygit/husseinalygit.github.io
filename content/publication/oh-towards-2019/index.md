---
title: Towards Reverse-Engineering Black-Box Neural Networks
authors:
- Seong Joon Oh
- Bernt Schiele
- Mario Fritz
date: '2019-01-01'
publishDate: '2023-11-24T10:39:01.328881Z'
publication_types:
- chapter
publication: '*Explainable AI: Interpreting, Explaining and Visualizing Deep Learning*'
doi: 10.1007/978-3-030-28954-6_7
abstract: Much progress in interpretable AI is built around scenarios where the user,
  one who interprets the model, has a full ownership of the model to be diagnosed.
  The user either owns the training data and computing resources to train an interpretable
  model herself or owns a full access to an already trained model to be interpreted
  post-hoc. In this chapter, we consider a less investigated scenario of diagnosing
  black-box neural networks, where the user can only send queries and read off outputs.
  Black-box access is a common deployment mode for many public and commercial models,
  since internal details, such as architecture, optimisation procedure, and training
  data, can be proprietary and aggravate their vulnerability to attacks like adversarial
  examples. We propose a method for exposing internals of black-box models and show
  that the method is surprisingly effective at inferring a diverse set of internal
  information. We further show how the exposed internals can be exploited to strengthen
  adversarial examples against the model. Our work starts an important discussion
  on the security implications of diagnosing deployed models with limited accessibility.
  The code is available at goo.gl/MbYfsv.
tags:
- Machine Learning
- Security
- Black box
- Explainability
links:
- name: URL
  url: https://doi.org/10.1007/978-3-030-28954-6_7
---
