---
title: Exploiting Unintended Feature Leakage in Collaborative Learning
authors:
- Luca Melis
- Congzheng Song
- Emiliano De Cristofaro
- Vitaly Shmatikov
date: '2019-05-01'
publishDate: '2023-11-24T10:39:00.324770Z'
publication_types:
- paper-conference
publication: '*2019 IEEE Symposium on Security and Privacy (SP)*'
doi: 10.1109/SP.2019.00029
abstract: Collaborative machine learning and related techniques such as federated
  learning allow multiple participants, each with his own training dataset, to build
  a joint model by training locally and periodically exchanging model updates. We
  demonstrate that these updates leak unintended information about participants' training
  data and develop passive and active inference attacks to exploit this leakage. First,
  we show that an adversarial participant can infer the presence of exact data points
  – for example, specific locations – in others' training data (i.e., membership inference).
  Then, we show how this adversary can infer properties that hold only for a subset
  of the training data and are independent of the properties that the joint model
  aims to capture. For example, he can infer when a specific person first appears
  in the photos used to train a binary gender classifier. We evaluate our attacks
  on a variety of tasks, datasets, and learning configurations, analyze their limitations,
  and discuss possible defenses.
tags:
- Data models
- Training
- Computational modeling
- Task analysis
- Servers
- security
- Collaborative work
- Training data
- deep-learning
- privacy
- collaborative-learning
- inference-attacks
---
