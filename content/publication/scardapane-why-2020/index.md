---
title: Why Should We Add Early Exits to Neural Networks?
authors:
- Simone Scardapane
- Michele Scarpiniti
- Enzo Baccarelli
- Aurelio Uncini
date: '2020-09-01'
publishDate: '2023-11-24T10:39:01.046716Z'
publication_types:
- article-journal
doi: 10.1007/s12559-020-09734-4
abstract: Deep neural networks are generally designed as a stack of differentiable
  layers, in which a prediction is obtained only after running the full stack. Recently,
  some contributions have proposed techniques to endow the networks with early exits,
  allowing to obtain predictions at intermediate points of the stack. These multi-output
  networks have a number of advantages, including (i) significant reductions of the
  inference time, (ii) reduced tendency to overfitting and vanishing gradients, and
  (iii) capability of being distributed over multi-tier computation platforms. In
  addition, they connect to the wider themes of biological plausibility and layered
  cognitive reasoning. In this paper, we provide a comprehensive introduction to this
  family of neural networks, by describing in a unified fashion the way these architectures
  can be designed, trained, and actually deployed in time-constrained scenarios. We
  also describe in-depth their application scenarios in 5G and Fog computing environments,
  as long as some of the open research questions connected to them.
tags:
- Deep learning
- Fog computing
- Conditional computation
- Distributed optimization
- Early exit
links:
- name: URL
  url: https://doi.org/10.1007/s12559-020-09734-4
---
